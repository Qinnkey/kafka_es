#spring.data.elasticsearch.cluster-name=java-es
#spring.data.elasticsearch.cluster-nodes=192.168.0.241:9300,192.168.0.251:9300,192.168.0.254:9300

#spring.data.elasticsearch.cluster-name=test
#spring.data.elasticsearch.cluster-nodes=192.168.56.101:9300
#
##elasticsearch日志存储目录
#spring.data.elasticsearch.properties.path.logs=./elasticsearch/log 
##elasticsearch数据存储目录
#spring.data.elasticsearch.properties.path.data=./elasticsearch/data
#spring.data.elasticsearch.repositories.enabled=true 


server.port=8004

#连接mysql
spring.datasource.url=jdbc:mysql://localhost:3306/pachong?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true
spring.datasource.username=root
spring.datasource.password=123456
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5Dialect
mybatis.mapper-locations=classpath:/mapper/*.xml


spring.elasticsearch.cluster-name=es-java
spring.elasticsearch..cluster-nodes=192.168.0.241:9300,192.168.0.251:9300,192.168.0.254:9300


#kafka相关配置,ip地址
spring.kafka.bootstrap-servers=192.168.0.251:9092,192.168.0.242:9092
#设置一个默认组
spring.kafka.consumer.group-id=test-consumer-group
#key-value序列化反序列化
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#每次fetch请求时，server应该返回的最小字节数
spring.kafka.consumer.fetch-min-size=100
spring.kafka.consumer.max-poll-records=3000
#关闭自动提交
spring.kafka.consumer.auto-commit-interval=1000
spring.kafka.consumer.enable-auto-commit=false

#配置 latest是消费新生产的数据， earliest是每个分区从头开始消费，none 当该topic下所有分区中存在未提交的offset时，抛出异常。
spring.kafka.consumer.auto-offset-reset=earliest

#设置消息的延迟
spring.kafka.properties.session.timeout.ms=15000
#服务器为获取请求返回的最大数据量。
spring.kafka.properties.fetch.max.bytes=9242880
#服务器返回的最大的字节数
spring.kafka.properties.max.partition.fetch.bytes=80485760

#设置监听器的并行数量
spring.kafka.listener.concurrency = 10
#设置监听得到数据的延迟
spring.kafka.listener.poll-timeout=30000
#提交的模式， record是每处理一条commit一次，batch是每一次poll的时候批量提交，count是累积多少次去提交一个和ack-count使用，而 count_time是谁先满足条件就去提交
#这个模式和spring.kafka.listener.ack-time和ack-count配合使用


#用来增加缓存等信息
spring.kafka.properties.receive.buffer.bytes =88008888
spring.kafka.properties.send.buffer.bytes=88008888






#用于kafka消费端的索引和类型
elasticsearch.index=product
elasticsearch.type=jd

http.token = 0
kafka.insert.topic = product_insert
kafka.update.topic = product_update
kafka.delete.topic = product_delete